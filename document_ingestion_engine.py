"""
DOCUMENT INGESTION ENGINE
Created: February 2, 2026
Last Updated: February 26, 2026 - COMPLETE REWRITE: Proper extractors for all Word document types

CHANGELOG:
- February 26, 2026: COMPLETE REWRITE - Real Knowledge Extraction
  * PROBLEM: All extractors were shallow. Contracts, implementation manuals, lessons learned,
    and scope of work documents were ingested but almost nothing was actually extracted.
    The generic fallback just recorded "a document was ingested" with a content_length.
    Zero patterns, zero actionable knowledge.
  * SOLUTION: Rebuilt all extractors against the actual file formats observed in production.
    Files on Render are Markdown-formatted text (not binary .docx). All regex patterns
    verified against real file content before writing.
  * CONTRACT EXTRACTOR: Extracts client name, total fee, payment schedule (milestone +
    amount pairs), engagement weeks, interest rate, termination formula.
  * IMPLEMENTATION MANUAL EXTRACTOR: Extracts schedule patterns mentioned (2-2-3, 2-3-2,
    4-3, DuPont, Panama, Pitman, 3-on-3-off, Continental, etc.), shift lengths (8hr/10hr/12hr),
    shift start times, facility/product context, appendix topics, employee selection process.
  * LESSONS LEARNED EXTRACTOR: Parses all 19 lessons individually. Each lesson captures:
    title, category, situation/trigger, key principle, real examples, full text for search.
    Lessons stored as individual searchable patterns with category tagging.
  * SCOPE OF WORK EXTRACTOR: Extracts client, weekly deliverables by week number, total
    cost, follow-up commitments, pre-project data collection requirements.
  * GENERAL WORD DOC EXTRACTOR: Improved fallback that extracts bold headings, dollar
    amounts, schedule patterns, and section structure from any Word doc.
  * PowerPoint and Excel extractors: Stubbed with clear TODO markers â€” will be built
    once sample files are provided.
  * All dollar amounts: handles Markdown-escaped format (\\$) used by Pandoc conversion.
  * NOTE: KNOWLEDGE_DB_PATH env var usage unchanged â€” persistent DB separation intact.

- February 3, 2026: CRITICAL FIX - Use KNOWLEDGE_DB_PATH env var for persistent storage
- February 2, 2026: Original file created

SUPPORTED DOCUMENT TYPES:
  Word Documents (text/Markdown format on server):
    - contract          â†’ fee, payment schedule, client, engagement terms
    - implementation_manual â†’ schedule patterns, shift details, facility context
    - lessons_learned   â†’ all lessons with category, principle, situation, examples
    - scope_of_work     â†’ weekly deliverables, cost, client, pre-project requirements
    - general_word      â†’ fallback for any other Word doc

  PowerPoint (TODO - awaiting sample files):
    - oaf               â†’ Operations Assessment findings
    - eaf               â†’ Employee Assessment findings
    - implementation_pptâ†’ Implementation presentation content

  Excel (TODO - awaiting sample files):
    - schedule_pattern  â†’ Shift schedule grid data
    - data_file         â†’ Labor/overtime/cost data

Author: Jim @ Shiftwork Solutions LLC (managed by Claude Sonnet 4)
"""

import hashlib
import json
import re
import os
from datetime import datetime
from typing import Dict, List, Any, Optional
import sqlite3


# All known schedule pattern names for detection across document types
KNOWN_SCHEDULE_PATTERNS = [
    '2-2-3', '2-3-2', '4-3', '4/3', 'DuPont', 'Panama', 'Pitman',
    '3-on-3-off', 'Continental', '5-2-5-3', '5&2', '5/2', 'Rotating 8',
    '4x10', '4x12', '3x12', '12-hour', '8-hour', '10-hour',
    'fixed days off', 'rotating', 'every other weekend'
]

# Dollar amount pattern for Markdown-escaped files (Pandoc converts $ to \$)
DOLLAR_PATTERN = re.compile(r'\\{1,2}\$([0-9,]+)')

# Engagement week pattern
WEEK_PATTERN = re.compile(r'Week\s+(\d+)', re.IGNORECASE)


class DocumentIngestor:
    """
    Ingests documents and extracts knowledge permanently to database.
    Rebuilt February 26, 2026 with proper extractors for all Word document types.
    """

    def __init__(self, db_path=None):
        # CRITICAL: Use environment variable for persistent storage
        if db_path is None:
            db_path = os.environ.get('KNOWLEDGE_DB_PATH', 'swarm_intelligence.db')
        self.db_path = db_path
        print(f"ðŸ“š Knowledge Database: {self.db_path}")
        self._ensure_tables()

    def _ensure_tables(self):
        """Create knowledge persistence tables if they don't exist"""
        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_extracts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_type TEXT NOT NULL,
                document_name TEXT,
                extracted_data TEXT NOT NULL,
                client TEXT,
                industry TEXT,
                project_type TEXT,
                source_hash TEXT UNIQUE,
                extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                file_size INTEGER,
                metadata TEXT
            )
        ''')

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learned_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_type TEXT NOT NULL,
                pattern_name TEXT,
                pattern_data TEXT NOT NULL,
                confidence REAL DEFAULT 0.5,
                supporting_documents INTEGER DEFAULT 1,
                first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                metadata TEXT
            )
        ''')

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ingestion_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_name TEXT,
                document_type TEXT,
                status TEXT,
                patterns_extracted INTEGER DEFAULT 0,
                insights_extracted INTEGER DEFAULT 0,
                error_message TEXT,
                ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        db.commit()
        db.close()

    # =========================================================================
    # MAIN INGEST ENTRY POINT
    # =========================================================================

    def ingest_document(self, content: str, document_type: str,
                        metadata: Dict = None) -> Dict[str, Any]:
        """
        Ingest a document and extract knowledge.

        Args:
            content: Full document content (text/Markdown)
            document_type: One of: contract, implementation_manual, lessons_learned,
                           scope_of_work, general_word, oaf, eaf, implementation_ppt,
                           schedule_pattern, data_file
            metadata: Optional dict with client, industry, project_type, document_name

        Returns:
            Ingestion result dict with counts and status message
        """
        metadata = metadata or {}
        document_name = metadata.get('document_name', 'Untitled')

        # Deduplicate via content hash
        content_hash = hashlib.md5(content.encode()).hexdigest()
        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()
        cursor.execute('SELECT id FROM knowledge_extracts WHERE source_hash = ?', (content_hash,))
        if cursor.fetchone():
            db.close()
            return {
                'success': True,
                'already_ingested': True,
                'message': f'{document_name} already in knowledge base'
            }
        db.close()

        # Route to correct extractor
        if document_type == 'contract':
            extracted = self._extract_from_contract(content, metadata)
        elif document_type == 'implementation_manual':
            extracted = self._extract_from_implementation_manual(content, metadata)
        elif document_type == 'lessons_learned':
            extracted = self._extract_from_lessons_learned(content, metadata)
        elif document_type == 'scope_of_work':
            extracted = self._extract_from_scope_of_work(content, metadata)
        elif document_type == 'oaf':
            extracted = self._extract_from_oaf(content, metadata)
        elif document_type == 'eaf':
            extracted = self._extract_from_eaf(content, metadata)
        elif document_type == 'implementation_ppt':
            extracted = self._extract_from_implementation_ppt(content, metadata)
        elif document_type == 'schedule_pattern':
            extracted = self._extract_from_schedule_pattern(content, metadata)
        elif document_type == 'data_file':
            extracted = self._extract_from_data_file(content, metadata)
        else:
            # Fallback: try to detect type from content, else use improved generic
            extracted = self._extract_general_word_doc(content, metadata)

        # Auto-detect client from content if not in metadata
        if not metadata.get('client'):
            detected_client = self._detect_client(content, extracted)
            if detected_client:
                metadata['client'] = detected_client
                extracted['detected_client'] = detected_client

        # Store to database
        self._store_extraction(
            document_type=document_type,
            document_name=document_name,
            extracted_data=extracted,
            source_hash=content_hash,
            metadata=metadata,
            client=metadata.get('client', ''),
            industry=metadata.get('industry', '')
        )

        # Update cumulative learned patterns
        patterns_added = self._update_cumulative_patterns(extracted)

        # Log ingestion
        self._log_ingestion(
            document_name=document_name,
            document_type=document_type,
            patterns_extracted=len(extracted.get('patterns', [])),
            insights_extracted=len(extracted.get('insights', []))
        )

        totals = self.get_knowledge_base_stats()

        return {
            'success': True,
            'document_name': document_name,
            'document_type': document_type,
            'patterns_extracted': len(extracted.get('patterns', [])),
            'insights_extracted': len(extracted.get('insights', [])),
            'patterns_added_to_kb': patterns_added,
            'total_kb_size': totals['total_extracts'],
            'total_patterns': totals['total_patterns'],
            'highlights': extracted.get('highlights', []),
            'message': (
                f"âœ… Ingested {document_name}. "
                f"Extracted {len(extracted.get('patterns', []))} patterns, "
                f"{len(extracted.get('insights', []))} insights. "
                f"Knowledge base now has {totals['total_patterns']} total patterns."
            )
        }

    # =========================================================================
    # WORD DOCUMENT EXTRACTORS
    # =========================================================================

    def _extract_from_contract(self, content: str, metadata: Dict) -> Dict:
        """
        Extract knowledge from contract/engagement agreement.

        Extracts: client name, total fee, payment milestones, engagement weeks,
        interest rate, termination formula, key terms.
        """
        extracted = {
            'patterns': [],
            'insights': [],
            'highlights': [],
            'document_category': 'contract'
        }

        # --- Client name ---
        client_match = re.search(
            r'\*\*([A-Z][A-Z\s&,\.\-]+)\*\*\s*\(the\s+["\']?Client["\']?\)',
            content
        )
        client = client_match.group(1).strip() if client_match else metadata.get('client', '')

        # --- Total fee ---
        fee_match = re.search(r'fixed sum of\\+\$([0-9,]+)', content)
        total_fee = fee_match.group(1).replace(',', '') if fee_match else None

        # --- Payment schedule ---
        payments = re.findall(
            r'Due upon\s+([^\:]+):\s*\\+\$([0-9,]+)',
            content
        )
        payment_schedule = [
            {'milestone': m.strip(), 'amount': int(a.replace(',', ''))}
            for m, a in payments
        ]

        # --- Engagement weeks ---
        weeks = sorted(set(int(w) for w in WEEK_PATTERN.findall(content)))
        max_week = max(weeks) if weeks else None

        # --- Interest rate ---
        interest_match = re.search(r'(\d+)%\s+per year on balances unpaid', content)
        interest_rate = int(interest_match.group(1)) if interest_match else 11

        # --- Termination formula ---
        formula_match = re.search(
            r'Consulting Fees Due\s*=\s*([^\n]+)',
            content
        )
        termination_formula = formula_match.group(1).strip() if formula_match else None

        # --- Expense policy ---
        expense_match = re.search(
            r'receipts for expenses greater than\\+\$(\d+)',
            content
        )
        expense_threshold = int(expense_match.group(1)) if expense_match else 40

        # --- Build structured insight ---
        contract_data = {
            'client': client,
            'total_fee': int(total_fee) if total_fee else None,
            'payment_schedule': payment_schedule,
            'engagement_weeks': max_week,
            'interest_rate_pct': interest_rate,
            'termination_formula': termination_formula,
            'expense_receipt_threshold': expense_threshold,
            'ip_ownership': 'consultant' if 'provided or developed by Consultant' in content else 'unknown'
        }

        extracted['insights'].append({
            'type': 'contract_terms',
            'data': contract_data
        })

        # --- Patterns ---
        if total_fee:
            extracted['patterns'].append({
                'type': 'engagement_fee',
                'name': f'fee_{total_fee}',
                'data': {
                    'fee': int(total_fee),
                    'client': client,
                    'weeks': max_week
                },
                'confidence': 0.95
            })

        if payment_schedule:
            extracted['patterns'].append({
                'type': 'payment_structure',
                'name': f'payment_milestones_{len(payment_schedule)}',
                'data': payment_schedule,
                'confidence': 0.95
            })

        extracted['patterns'].append({
            'type': 'contract_terms',
            'name': 'interest_rate',
            'data': {'rate_pct': interest_rate, 'days_before_interest': 30},
            'confidence': 1.0
        })

        # --- Highlights for UI ---
        if client:
            extracted['highlights'].append(f"Client: {client}")
        if total_fee:
            extracted['highlights'].append(f"Total Fee: ${int(total_fee):,}")
        if payment_schedule:
            for p in payment_schedule:
                extracted['highlights'].append(
                    f"Payment: ${p['amount']:,} due upon {p['milestone']}"
                )
        if max_week:
            extracted['highlights'].append(f"Engagement Duration: {max_week} weeks")

        return extracted

    def _extract_from_implementation_manual(self, content: str, metadata: Dict) -> Dict:
        """
        Extract knowledge from implementation manual.

        Extracts: schedule patterns presented, shift lengths, shift start times,
        facility/department context, appendix structure, employee selection process,
        pay calculation approaches.
        """
        extracted = {
            'patterns': [],
            'insights': [],
            'highlights': [],
            'document_category': 'implementation_manual'
        }

        # --- Schedule patterns mentioned ---
        found_patterns = []
        for pattern in KNOWN_SCHEDULE_PATTERNS:
            if pattern in content:
                found_patterns.append(pattern)

        if found_patterns:
            extracted['patterns'].append({
                'type': 'schedule_patterns_presented',
                'name': 'schedules_in_manual',
                'data': found_patterns,
                'confidence': 0.9
            })
            extracted['highlights'].append(f"Schedule Patterns: {', '.join(found_patterns)}")

        # --- Shift lengths ---
        shift_lengths = list(set(re.findall(r'(\d+)-hour shifts?', content, re.IGNORECASE)))
        if shift_lengths:
            extracted['patterns'].append({
                'type': 'shift_lengths',
                'name': 'hours_per_shift',
                'data': [int(h) for h in shift_lengths],
                'confidence': 0.95
            })
            extracted['highlights'].append(f"Shift Lengths: {', '.join(shift_lengths)}-hour")

        # --- Shift start times ---
        times = list(set(re.findall(r'(\d{1,2}:\d{2}\s*(?:am|pm))', content, re.IGNORECASE)))
        if times:
            extracted['insights'].append({
                'type': 'shift_start_times',
                'times': times
            })

        # --- Appendix topics (tells us what financial analysis was done) ---
        appendices = re.findall(r'Appendix\s+[A-Z][:\s\-]+([A-Za-z][^\n\*\[\\]+)', content)
        appendices = [a.strip() for a in appendices if len(a.strip()) > 3]
        # Deduplicate
        seen = set()
        unique_appendices = []
        for a in appendices:
            if a not in seen:
                seen.add(a)
                unique_appendices.append(a)

        if unique_appendices:
            extracted['insights'].append({
                'type': 'financial_analyses_included',
                'appendices': unique_appendices
            })
            extracted['highlights'].append(f"Appendices: {', '.join(unique_appendices[:4])}")

        # --- Employee selection process ---
        if 'Preference Form' in content or 'preference form' in content:
            extracted['patterns'].append({
                'type': 'employee_involvement',
                'name': 'preference_form_used',
                'data': {'method': 'written_preference_form'},
                'confidence': 0.9
            })

        if '24/7' in content:
            extracted['patterns'].append({
                'type': 'operation_type',
                'name': '24_7_coverage',
                'data': {'is_24_7': True},
                'confidence': 0.95
            })
            extracted['highlights'].append("Operation Type: 24/7 coverage")

        # --- Section headings (structural knowledge) ---
        headings = re.findall(r'^#+ (.+)$', content, re.MULTILINE)
        headings = [h.strip().replace('*', '') for h in headings if len(h.strip()) > 3]
        if headings:
            extracted['insights'].append({
                'type': 'manual_structure',
                'sections': headings[:20]
            })

        # --- Client/facility context from metadata ---
        if metadata.get('client'):
            extracted['insights'].append({
                'type': 'client_context',
                'client': metadata['client'],
                'industry': metadata.get('industry', 'unknown')
            })

        return extracted

    def _extract_from_lessons_learned(self, content: str, metadata: Dict) -> Dict:
        """
        Extract knowledge from lessons learned document.

        Parses each lesson individually capturing: title, category, situation/trigger,
        key principle, real examples, full searchable text.
        Each lesson becomes its own searchable pattern in the knowledge base.
        """
        extracted = {
            'patterns': [],
            'insights': [],
            'highlights': [],
            'document_category': 'lessons_learned'
        }

        # Split into individual lesson blocks
        lesson_blocks = re.split(r'(?=### Lesson #\d+)', content)
        lesson_blocks = [b.strip() for b in lesson_blocks if b.strip().startswith('### Lesson')]

        # Map lessons back to their categories
        # Build category map by finding ## headings before each lesson
        category_map = {}
        current_category = 'General'
        for line in content.split('\n'):
            line = line.strip()
            if line.startswith('## ') and 'Categories' not in line and 'How to Add' not in line and 'Notes for' not in line:
                current_category = line.replace('## ', '').strip()
            elif line.startswith('### Lesson'):
                title_match = re.match(r'### (Lesson #\d+)', line)
                if title_match:
                    category_map[title_match.group(1)] = current_category

        lessons_extracted = []

        for block in lesson_blocks:
            # Title
            title_match = re.match(r'### (Lesson #(\d+)[^\n]+)', block)
            if not title_match:
                continue

            full_title = title_match.group(1).strip()
            lesson_num = int(title_match.group(2))
            lesson_name = re.sub(r'Lesson #\d+:\s*', '', full_title).strip()

            # Category
            lesson_key = f"Lesson #{lesson_num}"
            category = category_map.get(lesson_key, 'General')

            # Situation/Trigger
            situation_match = re.search(
                r'\*\*Situation[^*]*\*\*\s*\n+(.*?)(?=\n\*\*|\Z)',
                block, re.DOTALL
            )
            situation = situation_match.group(1).strip()[:300] if situation_match else ''

            # Key Principle
            principle_match = re.search(
                r'\*\*Key Principle[:\*\*\s]+(.+?)(?=\n---|\n###|\Z)',
                block, re.DOTALL
            )
            principle = principle_match.group(1).strip()[:400] if principle_match else ''

            # Real examples (look for "Real Example" sections)
            examples = re.findall(
                r'\*\*Real Example[^\n]*\*\*\s*\n+(.*?)(?=\n\*\*|\n---|\Z)',
                block, re.DOTALL
            )
            examples = [e.strip()[:200] for e in examples]

            lesson_data = {
                'lesson_number': lesson_num,
                'title': full_title,
                'lesson_name': lesson_name,
                'category': category,
                'situation': situation,
                'key_principle': principle,
                'real_examples': examples,
                'full_text': block[:1000]  # First 1000 chars for search
            }

            lessons_extracted.append(lesson_data)

            # Each lesson becomes its own searchable pattern
            extracted['patterns'].append({
                'type': 'consulting_lesson',
                'name': f'lesson_{lesson_num}_{re.sub(r"[^a-z0-9]", "_", lesson_name.lower())[:40]}',
                'data': lesson_data,
                'confidence': 1.0
            })

        # Summary insight
        categories_covered = list(set(l['category'] for l in lessons_extracted))
        extracted['insights'].append({
            'type': 'lessons_learned_summary',
            'total_lessons': len(lessons_extracted),
            'categories': categories_covered,
            'lessons': [
                {'number': l['lesson_number'], 'name': l['lesson_name'], 'category': l['category']}
                for l in lessons_extracted
            ]
        })

        extracted['highlights'].append(f"Total Lessons: {len(lessons_extracted)}")
        extracted['highlights'].append(f"Categories: {', '.join(categories_covered)}")

        return extracted

    def _extract_from_scope_of_work(self, content: str, metadata: Dict) -> Dict:
        """
        Extract knowledge from scope of work document.

        Extracts: client name, total cost, weekly deliverables by week,
        pre-project data requirements, follow-up commitments.
        """
        extracted = {
            'patterns': [],
            'insights': [],
            'highlights': [],
            'document_category': 'scope_of_work'
        }

        # --- Client (second bold item after "Attachment A: Scope of Work") ---
        bold_items = re.findall(r'\*\*([^*\n]+)\*\*', content)
        client = None
        for item in bold_items:
            item = item.strip()
            # Skip known non-client bold items
            if item in ('Attachment A: Scope of Work', 'Summary:', 'Detailed Description:'):
                continue
            if re.search(r'[A-Z]{3,}', item) and len(item) < 60:
                client = item
                break

        # --- Total cost ---
        cost_match = re.search(r'\\+\$([0-9,]+)\s*\+?\s*travel', content, re.IGNORECASE)
        if not cost_match:
            cost_match = re.search(r'Project Cost[^$]*\\+\$([0-9,]+)', content, re.IGNORECASE)
        total_cost = int(cost_match.group(1).replace(',', '')) if cost_match else None

        # --- Weekly deliverables ---
        weekly_deliverables = {}
        week_sections = re.finditer(
            r'\*\*Week\s+(\d+)[:\*\s]+(.*?)(?=\*\*Week\s+\d+|\*\*Follow|\*\*Project Cost|\*\*Detailed|\Z)',
            content, re.DOTALL
        )
        for match in week_sections:
            week_num = int(match.group(1))
            week_content = match.group(2).strip()
            # Extract bullet points
            bullets = re.findall(r'-\s+(.+)', week_content)
            bullets = [b.strip() for b in bullets if b.strip()]
            if bullets:
                weekly_deliverables[f'week_{week_num}'] = bullets

        # --- Pre-project data requirements ---
        pre_project = []
        pre_match = re.search(
            r'Pre-Project[^\n]*\n+(.*?)(?=\*\*Project Week 1|\Z)',
            content, re.DOTALL | re.IGNORECASE
        )
        if pre_match:
            pre_bullets = re.findall(r'-\s+(.+)', pre_match.group(1))
            pre_project = [b.strip() for b in pre_bullets if b.strip()]

        # --- Follow-up ---
        followup_match = re.search(
            r'\*\*Follow[- ]?up[:\*\s]+(.*?)(?=\*\*Project Cost|\*\*Detailed|\Z)',
            content, re.DOTALL | re.IGNORECASE
        )
        followup = []
        if followup_match:
            followup_bullets = re.findall(r'-\s+(.+)', followup_match.group(1))
            followup = [b.strip() for b in followup_bullets if b.strip()]

        sow_data = {
            'client': client,
            'total_cost': total_cost,
            'weekly_deliverables': weekly_deliverables,
            'pre_project_data_requirements': pre_project,
            'follow_up_commitments': followup,
            'total_weeks': len(weekly_deliverables)
        }

        extracted['insights'].append({
            'type': 'scope_of_work',
            'data': sow_data
        })

        # --- Patterns ---
        if total_cost:
            extracted['patterns'].append({
                'type': 'engagement_cost',
                'name': f'sow_cost_{total_cost}',
                'data': {'cost': total_cost, 'client': client},
                'confidence': 0.95
            })

        if weekly_deliverables:
            extracted['patterns'].append({
                'type': 'project_structure',
                'name': f'week_by_week_{len(weekly_deliverables)}_weeks',
                'data': weekly_deliverables,
                'confidence': 0.9
            })

        # --- Highlights ---
        if client:
            extracted['highlights'].append(f"Client: {client}")
        if total_cost:
            extracted['highlights'].append(f"Project Cost: ${total_cost:,} + travel")
        if weekly_deliverables:
            extracted['highlights'].append(f"Project Duration: {len(weekly_deliverables)} weeks")
        if pre_project:
            extracted['highlights'].append(f"Pre-project data items: {len(pre_project)}")

        return extracted

    def _extract_general_word_doc(self, content: str, metadata: Dict) -> Dict:
        """
        Improved generic extractor for any Word document not matched by a specific extractor.

        Extracts: bold headings, dollar amounts, schedule patterns, section structure,
        any client/company names in bold, key statistics.
        """
        extracted = {
            'patterns': [],
            'insights': [],
            'highlights': [],
            'document_category': 'general_word'
        }

        # --- Bold headings / section titles ---
        headings = re.findall(r'\*\*([^*\n]{5,80})\*\*', content)
        headings = [h.strip() for h in headings if not h.strip().endswith(':')]
        if headings:
            extracted['insights'].append({
                'type': 'document_structure',
                'headings': headings[:30]
            })

        # --- Dollar amounts ---
        amounts = [int(a.replace(',', '')) for a in DOLLAR_PATTERN.findall(content)]
        if amounts:
            extracted['insights'].append({
                'type': 'financial_figures',
                'amounts': amounts
            })
            extracted['highlights'].append(
                f"Dollar amounts found: {', '.join(f'${a:,}' for a in sorted(set(amounts))[:5])}"
            )

        # --- Schedule patterns ---
        found_patterns = [p for p in KNOWN_SCHEDULE_PATTERNS if p in content]
        if found_patterns:
            extracted['patterns'].append({
                'type': 'schedule_patterns_mentioned',
                'name': 'schedules_referenced',
                'data': found_patterns,
                'confidence': 0.8
            })
            extracted['highlights'].append(f"Schedule patterns: {', '.join(found_patterns)}")

        # --- Percentage figures ---
        percentages = re.findall(r'(\d+(?:\.\d+)?)\s*%', content)
        percentages = [float(p) for p in percentages]
        if percentages:
            extracted['insights'].append({
                'type': 'percentage_figures',
                'values': sorted(set(percentages))[:10]
            })

        # --- Content summary ---
        extracted['insights'].append({
            'type': 'document_summary',
            'content_length': len(content),
            'word_count': len(content.split()),
            'document_name': metadata.get('document_name', 'unknown')
        })

        return extracted

    # =========================================================================
    # POWERPOINT EXTRACTORS (TODO - awaiting sample files)
    # =========================================================================

    def _extract_from_oaf(self, content: str, metadata: Dict) -> Dict:
        """
        Extract knowledge from Operations Assessment (OAF) presentation.

        TODO: Full extractor to be built once sample PowerPoint files are provided.
        Current implementation extracts what it can from text content.
        """
        extracted = {
            'patterns': [],
            'insights': [],
            'highlights': [],
            'document_category': 'oaf'
        }

        # Extract overtime percentage if present
        overtime_match = re.search(r'overtime[:\s]+(\d+(?:\.\d+)?)\s*%', content, re.IGNORECASE)
        if overtime_match:
            ot_pct = float(overtime_match.group(1))
            extracted['insights'].append({
                'type': 'operational_metric',
                'metric': 'overtime_percentage',
                'value': ot_pct,
                'client': metadata.get('client', 'unknown')
            })
            extracted['highlights'].append(f"Overtime: {ot_pct}%")
            extracted['patterns'].append({
                'type': 'overtime_level',
                'name': f'overtime_{int(ot_pct)}_pct',
                'data': {'overtime_pct': ot_pct, 'client': metadata.get('client')},
                'confidence': 0.85
            })

        # Extract headcount if present
        headcount_match = re.search(r'(\d+)\s*(?:employees?|operators?|workers?)', content, re.IGNORECASE)
        if headcount_match:
            headcount = int(headcount_match.group(1))
            if 10 <= headcount <= 5000:  # Reasonable range
                extracted['insights'].append({
                    'type': 'workforce_size',
                    'headcount': headcount,
                    'client': metadata.get('client', 'unknown')
                })
                extracted['highlights'].append(f"Headcount: {headcount}")

        # Generic extraction as fallback
        generic = self._extract_general_word_doc(content, metadata)
        extracted['patterns'].extend(generic['patterns'])
        extracted['insights'].extend(generic['insights'])

        extracted['highlights'].append("NOTE: Full OAF extractor pending PowerPoint sample files")
        return extracted

    def _extract_from_eaf(self, content: str, metadata: Dict) -> Dict:
        """
        Extract knowledge from Employee Assessment (EAF) presentation.

        TODO: Full extractor to be built once sample PowerPoint files are provided.
        Current implementation extracts what it can from text content.
        """
        extracted = {
            'patterns': [],
            'insights': [],
            'highlights': [],
            'document_category': 'eaf'
        }

        # Satisfaction score
        satisfaction_match = re.search(
            r'satisfaction[:\s]+(\d+(?:\.\d+)?)\s*(?:/|out of)?\s*10',
            content, re.IGNORECASE
        )
        if satisfaction_match:
            score = float(satisfaction_match.group(1))
            extracted['insights'].append({
                'type': 'employee_satisfaction',
                'score': score,
                'client': metadata.get('client', 'unknown')
            })
            extracted['highlights'].append(f"Employee Satisfaction: {score}/10")
            extracted['patterns'].append({
                'type': 'satisfaction_score',
                'name': f'eaf_satisfaction_{int(score)}',
                'data': {'score': score, 'client': metadata.get('client')},
                'confidence': 0.85
            })

        # Survey response rate
        response_match = re.search(r'(\d+(?:\.\d+)?)\s*%\s*response', content, re.IGNORECASE)
        if response_match:
            rate = float(response_match.group(1))
            extracted['insights'].append({
                'type': 'survey_response_rate',
                'rate_pct': rate,
                'client': metadata.get('client', 'unknown')
            })
            extracted['highlights'].append(f"Survey Response Rate: {rate}%")

        generic = self._extract_general_word_doc(content, metadata)
        extracted['patterns'].extend(generic['patterns'])
        extracted['insights'].extend(generic['insights'])

        extracted['highlights'].append("NOTE: Full EAF extractor pending PowerPoint sample files")
        return extracted

    def _extract_from_implementation_ppt(self, content: str, metadata: Dict) -> Dict:
        """
        Extract knowledge from Implementation presentation.

        TODO: Full extractor to be built once sample PowerPoint files are provided.
        """
        extracted = self._extract_general_word_doc(content, metadata)
        extracted['document_category'] = 'implementation_ppt'
        extracted['highlights'].append("NOTE: Full implementation PPT extractor pending sample files")

        # Find schedule patterns as minimum
        found_patterns = [p for p in KNOWN_SCHEDULE_PATTERNS if p in content]
        if found_patterns:
            extracted['insights'].append({
                'type': 'schedules_implemented',
                'patterns': found_patterns,
                'client': metadata.get('client')
            })

        return extracted

    # =========================================================================
    # EXCEL EXTRACTORS (TODO - awaiting sample files)
    # =========================================================================

    def _extract_from_schedule_pattern(self, content: str, metadata: Dict) -> Dict:
        """
        Extract knowledge from Excel schedule pattern file.

        TODO: Full extractor to be built once sample Excel files are provided.
        Will parse shift grids, crew rotations, coverage matrices.
        """
        extracted = {
            'patterns': [],
            'insights': [],
            'highlights': ['NOTE: Full schedule pattern extractor pending Excel sample files'],
            'document_category': 'schedule_pattern'
        }

        # Basic pattern detection from any text content
        found_patterns = [p for p in KNOWN_SCHEDULE_PATTERNS if p in content]
        if found_patterns:
            extracted['patterns'].append({
                'type': 'schedule_pattern_defined',
                'name': 'excel_schedule_pattern',
                'data': {'patterns': found_patterns},
                'confidence': 0.8
            })

        extracted['insights'].append({
            'type': 'schedule_file_ingested',
            'document_name': metadata.get('document_name'),
            'patterns_found': found_patterns
        })

        return extracted

    def _extract_from_data_file(self, content: str, metadata: Dict) -> Dict:
        """
        Extract knowledge from Excel data file (labor hours, overtime, headcount, costs).

        TODO: Full extractor to be built once sample Excel files are provided.
        Will parse numeric data, calculate averages, identify trends.
        """
        extracted = {
            'patterns': [],
            'insights': [],
            'highlights': ['NOTE: Full data file extractor pending Excel sample files'],
            'document_category': 'data_file'
        }

        # Extract any numbers that look like percentages or hours
        percentages = re.findall(r'(\d+(?:\.\d+)?)\s*%', content)
        if percentages:
            extracted['insights'].append({
                'type': 'data_percentages',
                'values': [float(p) for p in percentages[:20]]
            })

        extracted['insights'].append({
            'type': 'data_file_ingested',
            'document_name': metadata.get('document_name'),
            'content_length': len(content)
        })

        return extracted

    # =========================================================================
    # HELPER METHODS
    # =========================================================================

    def _detect_client(self, content: str, extracted: Dict) -> Optional[str]:
        """
        Auto-detect client name from content if not provided in metadata.
        Looks in contract insights first, then scans for bold all-caps names.
        """
        # Check if contract extractor already found it
        for insight in extracted.get('insights', []):
            if insight.get('type') == 'contract_terms':
                return insight['data'].get('client')
            if insight.get('type') == 'scope_of_work':
                return insight['data'].get('client')

        # Scan for bold all-caps company names
        candidates = re.findall(r'\*\*([A-Z][A-Z\s&,\.\-]{3,40})\*\*', content)
        for candidate in candidates:
            candidate = candidate.strip()
            # Filter out known non-client phrases
            skip = {'SAMPLE', 'TABLE OF CONTENTS', 'APPENDIX', 'INTRODUCTION',
                    'SUMMARY', 'SCOPE OF WORK', 'NOTE', 'IMPORTANT', 'WARNING'}
            if candidate not in skip and len(candidate) >= 4:
                return candidate

        return None

    def _store_extraction(self, document_type: str, document_name: str,
                          extracted_data: Dict, source_hash: str, metadata: Dict,
                          client: str = '', industry: str = ''):
        """Store extracted knowledge in database"""
        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()

        cursor.execute('''
            INSERT INTO knowledge_extracts (
                document_type, document_name, extracted_data,
                client, industry, project_type, source_hash, metadata
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            document_type,
            document_name,
            json.dumps(extracted_data),
            client,
            industry,
            metadata.get('project_type'),
            source_hash,
            json.dumps(metadata)
        ))

        db.commit()
        db.close()

    def _update_cumulative_patterns(self, extracted: Dict) -> int:
        """
        Update cumulative learned patterns.
        Patterns grow stronger (higher confidence) with each supporting document.
        """
        patterns_added = 0

        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()

        for pattern in extracted.get('patterns', []):
            pattern_key_type = pattern.get('type', 'unknown')
            pattern_key_name = pattern.get('name', 'unknown')

            cursor.execute('''
                SELECT id, supporting_documents, confidence FROM learned_patterns
                WHERE pattern_type = ? AND pattern_name = ?
            ''', (pattern_key_type, pattern_key_name))

            existing = cursor.fetchone()

            if existing:
                pattern_id, support_count, current_confidence = existing
                new_support = support_count + 1
                # Confidence asymptotically approaches 1.0 with more supporting docs
                new_confidence = min(0.99, current_confidence + (0.05 / new_support))

                cursor.execute('''
                    UPDATE learned_patterns
                    SET supporting_documents = ?,
                        confidence = ?,
                        last_updated = ?,
                        pattern_data = ?
                    WHERE id = ?
                ''', (
                    new_support,
                    new_confidence,
                    datetime.now(),
                    json.dumps(pattern.get('data', {})),
                    pattern_id
                ))
            else:
                cursor.execute('''
                    INSERT INTO learned_patterns (
                        pattern_type, pattern_name, pattern_data, confidence
                    ) VALUES (?, ?, ?, ?)
                ''', (
                    pattern_key_type,
                    pattern_key_name,
                    json.dumps(pattern.get('data', {})),
                    pattern.get('confidence', 0.5)
                ))
                patterns_added += 1

        db.commit()
        db.close()

        return patterns_added

    def _log_ingestion(self, document_name: str, document_type: str,
                       patterns_extracted: int, insights_extracted: int,
                       error_message: str = None):
        """Log ingestion for tracking"""
        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()

        cursor.execute('''
            INSERT INTO ingestion_log (
                document_name, document_type, status,
                patterns_extracted, insights_extracted, error_message
            ) VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            document_name,
            document_type,
            'success' if not error_message else 'error',
            patterns_extracted,
            insights_extracted,
            error_message
        ))

        db.commit()
        db.close()

    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """Get knowledge base statistics"""
        db = sqlite3.connect(self.db_path)
        db.row_factory = sqlite3.Row
        cursor = db.cursor()

        cursor.execute('SELECT COUNT(*) as count FROM knowledge_extracts')
        total_extracts = cursor.fetchone()['count']

        cursor.execute('SELECT COUNT(*) as count FROM learned_patterns')
        total_patterns = cursor.fetchone()['count']

        cursor.execute('''
            SELECT document_type, COUNT(*) as count
            FROM knowledge_extracts
            GROUP BY document_type
        ''')
        by_type = {row['document_type']: row['count'] for row in cursor.fetchall()}

        cursor.execute('''
            SELECT document_name, document_type, status, patterns_extracted,
                   insights_extracted, ingested_at
            FROM ingestion_log
            WHERE status = 'success'
            ORDER BY ingested_at DESC
            LIMIT 10
        ''')
        recent = [dict(row) for row in cursor.fetchall()]

        db.close()

        return {
            'total_extracts': total_extracts,
            'total_patterns': total_patterns,
            'by_document_type': by_type,
            'recent_ingestions': recent
        }

    def query_knowledge(self, query: str, document_type: str = None,
                        client: str = None, limit: int = 10) -> List[Dict]:
        """
        Query the knowledge base for relevant extracts.

        Args:
            query: Search terms
            document_type: Optional filter by doc type
            client: Optional filter by client
            limit: Max results

        Returns:
            List of matching knowledge extracts
        """
        db = sqlite3.connect(self.db_path)
        db.row_factory = sqlite3.Row
        cursor = db.cursor()

        sql = 'SELECT * FROM knowledge_extracts WHERE 1=1'
        params = []

        if document_type:
            sql += ' AND document_type = ?'
            params.append(document_type)

        if client:
            sql += ' AND client LIKE ?'
            params.append(f'%{client}%')

        if query:
            sql += ' AND (document_name LIKE ? OR extracted_data LIKE ?)'
            params.extend([f'%{query}%', f'%{query}%'])

        sql += ' ORDER BY extracted_at DESC LIMIT ?'
        params.append(limit)

        rows = cursor.fetchall() if not params else db.execute(sql, params).fetchall()
        db.close()

        results = []
        for row in rows:
            item = dict(row)
            if item.get('extracted_data'):
                try:
                    item['extracted_data'] = json.loads(item['extracted_data'])
                except Exception:
                    pass
            results.append(item)

        return results


# =========================================================================
# SINGLETON
# =========================================================================

_ingestor = None

def get_document_ingestor() -> DocumentIngestor:
    """Get singleton document ingestor"""
    global _ingestor
    if _ingestor is None:
        _ingestor = DocumentIngestor()
    return _ingestor


# I did no harm and this file is not truncated
