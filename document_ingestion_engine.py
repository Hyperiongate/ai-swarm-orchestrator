"""
DOCUMENT INGESTION ENGINE 
Created: February 2, 2026
Last Updated: February 2, 2026

Extracts knowledge from documents and stores permanently in database.
No more file management - upload once, learn forever.

This is the "shoulders of giants" system - cumulative, persistent learning.

Author: Jim @ Shiftwork Solutions LLC (managed by Claude Sonnet 4)
"""

import hashlib
import json
import re
from datetime import datetime
from typing import Dict, List, Any, Optional
import sqlite3


class DocumentIngestor:
    """
    Ingests documents and extracts knowledge permanently to database.
    """
    
    def __init__(self, db_path='swarm_intelligence.db'):
        self.db_path = db_path
        self._ensure_tables()
    
    def _ensure_tables(self):
        """Create knowledge persistence tables"""
        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()
        
        # Raw extracted knowledge from each document
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_extracts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_type TEXT NOT NULL,
                document_name TEXT,
                extracted_data TEXT NOT NULL,
                client TEXT,
                industry TEXT,
                project_type TEXT,
                source_hash TEXT UNIQUE,
                extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                file_size INTEGER,
                metadata TEXT
            )
        ''')
        
        # Cumulative learned patterns (gets stronger with more documents)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS learned_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_type TEXT NOT NULL,
                pattern_name TEXT,
                pattern_data TEXT NOT NULL,
                confidence REAL DEFAULT 0.5,
                supporting_documents INTEGER DEFAULT 1,
                first_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                metadata TEXT
            )
        ''')
        
        # Ingestion log
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS ingestion_log (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                document_name TEXT,
                document_type TEXT,
                status TEXT,
                patterns_extracted INTEGER DEFAULT 0,
                insights_extracted INTEGER DEFAULT 0,
                error_message TEXT,
                ingested_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        db.commit()
        db.close()
    
    def ingest_document(self, content: str, document_type: str, 
                       metadata: Dict = None) -> Dict[str, Any]:
        """
        Ingest a document and extract knowledge.
        
        Args:
            content: Full document content
            document_type: Type (implementation_manual, oaf, eaf, lessons_learned, etc.)
            metadata: Optional metadata (client, industry, etc.)
            
        Returns:
            Ingestion results
        """
        metadata = metadata or {}
        document_name = metadata.get('document_name', 'Untitled')
        
        # Check if already ingested (via hash)
        content_hash = hashlib.md5(content.encode()).hexdigest()
        
        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()
        cursor.execute('SELECT id FROM knowledge_extracts WHERE source_hash = ?', (content_hash,))
        if cursor.fetchone():
            db.close()
            return {
                'success': True,
                'already_ingested': True,
                'message': f'{document_name} already in knowledge base'
            }
        db.close()
        
        # Extract knowledge based on document type
        if document_type == 'implementation_manual':
            extracted = self._extract_from_implementation_manual(content, metadata)
        elif document_type == 'lessons_learned':
            extracted = self._extract_from_lessons_learned(content, metadata)
        elif document_type == 'oaf':
            extracted = self._extract_from_oaf(content, metadata)
        elif document_type == 'eaf':
            extracted = self._extract_from_eaf(content, metadata)
        else:
            extracted = self._extract_generic(content, metadata)
        
        # Store extracted knowledge
        self._store_extraction(
            document_type=document_type,
            document_name=document_name,
            extracted_data=extracted,
            source_hash=content_hash,
            metadata=metadata
        )
        
        # Update cumulative patterns
        patterns_added = self._update_cumulative_patterns(extracted)
        
        # Log ingestion
        self._log_ingestion(
            document_name=document_name,
            document_type=document_type,
            patterns_extracted=len(extracted.get('patterns', [])),
            insights_extracted=len(extracted.get('insights', []))
        )
        
        # Get totals
        totals = self.get_knowledge_base_stats()
        
        return {
            'success': True,
            'document_name': document_name,
            'document_type': document_type,
            'patterns_extracted': len(extracted.get('patterns', [])),
            'insights_extracted': len(extracted.get('insights', [])),
            'patterns_added_to_kb': patterns_added,
            'total_kb_size': totals['total_extracts'],
            'total_patterns': totals['total_patterns'],
            'message': f'âœ… Ingested {document_name}. Knowledge base now has {totals["total_patterns"]} patterns.'
        }
    
    def _extract_from_implementation_manual(self, content: str, metadata: Dict) -> Dict:
        """Extract knowledge from implementation manual"""
        extracted = {
            'patterns': [],
            'insights': [],
            'structure': {}
        }
        
        # Extract sections
        sections = []
        lines = content.split('\n')
        current_section = None
        
        for line in lines:
            # Look for headers (all caps, or starts with ##, or numbered)
            if line.isupper() and len(line.strip()) > 5:
                sections.append(line.strip())
            elif line.startswith('##'):
                sections.append(line.replace('#', '').strip())
            elif re.match(r'^\d+\.\s+[A-Z]', line):
                sections.append(line.strip())
        
        if sections:
            extracted['structure']['sections'] = sections
            extracted['patterns'].append({
                'type': 'document_structure',
                'name': 'implementation_manual_sections',
                'data': sections,
                'confidence': 0.9
            })
        
        # Extract client/industry info
        if metadata.get('client'):
            extracted['insights'].append({
                'type': 'client_info',
                'client': metadata['client'],
                'industry': metadata.get('industry', 'unknown')
            })
        
        # Look for schedule pattern mentions
        schedule_patterns = ['2-2-3', 'DuPont', 'Panama', 'Pitman', '5&2', 'Continental']
        for pattern in schedule_patterns:
            if pattern in content:
                extracted['patterns'].append({
                    'type': 'schedule_pattern_used',
                    'name': pattern,
                    'confidence': 0.8
                })
        
        return extracted
    
    def _extract_from_lessons_learned(self, content: str, metadata: Dict) -> Dict:
        """Extract knowledge from lessons learned document"""
        extracted = {
            'patterns': [],
            'insights': []
        }
        
        # Extract lesson blocks
        lesson_blocks = re.findall(r'###\s*Lesson\s*#?\d*:?\s*(.+?)(?=###|$)', content, re.DOTALL)
        
        for lesson in lesson_blocks:
            # Extract key principles
            if 'Why It Matters' in lesson or 'Key Principle' in lesson:
                extracted['insights'].append({
                    'type': 'consulting_wisdom',
                    'content': lesson[:500],  # First 500 chars
                    'confidence': 1.0
                })
        
        extracted['patterns'].append({
            'type': 'lessons_count',
            'name': 'total_lessons_documented',
            'data': len(lesson_blocks),
            'confidence': 1.0
        })
        
        return extracted
    
    def _extract_from_oaf(self, content: str, metadata: Dict) -> Dict:
        """Extract knowledge from Operations Assessment"""
        extracted = {
            'patterns': [],
            'insights': []
        }
        
        # Look for operational metrics
        overtime_match = re.search(r'overtime[:\s]+(\d+)%', content, re.IGNORECASE)
        if overtime_match:
            extracted['insights'].append({
                'type': 'operational_metric',
                'metric': 'overtime',
                'value': int(overtime_match.group(1)),
                'client': metadata.get('client', 'unknown')
            })
        
        return extracted
    
    def _extract_from_eaf(self, content: str, metadata: Dict) -> Dict:
        """Extract knowledge from Employee Assessment"""
        extracted = {
            'patterns': [],
            'insights': []
        }
        
        # Extract satisfaction scores
        satisfaction_match = re.search(r'satisfaction[:\s]+(\d+(?:\.\d+)?)\s*(?:\/|out of)?\s*10', content, re.IGNORECASE)
        if satisfaction_match:
            score = float(satisfaction_match.group(1))
            extracted['insights'].append({
                'type': 'employee_satisfaction',
                'metric': 'overall_satisfaction',
                'score': score,
                'client': metadata.get('client', 'unknown'),
                'industry': metadata.get('industry', 'unknown')
            })
        
        return extracted
    
    def _extract_generic(self, content: str, metadata: Dict) -> Dict:
        """Extract generic knowledge"""
        return {
            'patterns': [],
            'insights': [{
                'type': 'document_ingested',
                'content_length': len(content),
                'metadata': metadata
            }]
        }
    
    def _store_extraction(self, document_type: str, document_name: str,
                         extracted_data: Dict, source_hash: str, metadata: Dict):
        """Store extracted knowledge in database"""
        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()
        
        cursor.execute('''
            INSERT INTO knowledge_extracts (
                document_type, document_name, extracted_data,
                client, industry, source_hash, metadata
            ) VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            document_type,
            document_name,
            json.dumps(extracted_data),
            metadata.get('client'),
            metadata.get('industry'),
            source_hash,
            json.dumps(metadata)
        ))
        
        db.commit()
        db.close()
    
    def _update_cumulative_patterns(self, extracted: Dict) -> int:
        """Update cumulative patterns - they get stronger with more supporting docs"""
        patterns_added = 0
        
        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()
        
        for pattern in extracted.get('patterns', []):
            pattern_key = f"{pattern['type']}:{pattern['name']}"
            
            # Check if pattern exists
            cursor.execute('''
                SELECT id, supporting_documents, confidence FROM learned_patterns
                WHERE pattern_type = ? AND pattern_name = ?
            ''', (pattern['type'], pattern['name']))
            
            existing = cursor.fetchone()
            
            if existing:
                # Update existing pattern - increase confidence
                pattern_id, support_count, current_confidence = existing
                new_support = support_count + 1
                # Confidence increases with more supporting docs (asymptotic to 1.0)
                new_confidence = min(0.99, current_confidence + (0.1 / new_support))
                
                cursor.execute('''
                    UPDATE learned_patterns
                    SET supporting_documents = ?,
                        confidence = ?,
                        last_updated = ?
                    WHERE id = ?
                ''', (new_support, new_confidence, datetime.now(), pattern_id))
            else:
                # New pattern
                cursor.execute('''
                    INSERT INTO learned_patterns (
                        pattern_type, pattern_name, pattern_data, confidence
                    ) VALUES (?, ?, ?, ?)
                ''', (
                    pattern['type'],
                    pattern['name'],
                    json.dumps(pattern.get('data', {})),
                    pattern.get('confidence', 0.5)
                ))
                patterns_added += 1
        
        db.commit()
        db.close()
        
        return patterns_added
    
    def _log_ingestion(self, document_name: str, document_type: str,
                      patterns_extracted: int, insights_extracted: int):
        """Log ingestion for tracking"""
        db = sqlite3.connect(self.db_path)
        cursor = db.cursor()
        
        cursor.execute('''
            INSERT INTO ingestion_log (
                document_name, document_type, status,
                patterns_extracted, insights_extracted
            ) VALUES (?, ?, ?, ?, ?)
        ''', (document_name, document_type, 'success', patterns_extracted, insights_extracted))
        
        db.commit()
        db.close()
    
    def get_knowledge_base_stats(self) -> Dict[str, Any]:
        """Get knowledge base statistics"""
        db = sqlite3.connect(self.db_path)
        db.row_factory = sqlite3.Row
        cursor = db.cursor()
        
        # Total documents ingested
        cursor.execute('SELECT COUNT(*) as count FROM knowledge_extracts')
        total_extracts = cursor.fetchone()['count']
        
        # Total patterns
        cursor.execute('SELECT COUNT(*) as count FROM learned_patterns')
        total_patterns = cursor.fetchone()['count']
        
        # By document type
        cursor.execute('''
            SELECT document_type, COUNT(*) as count
            FROM knowledge_extracts
            GROUP BY document_type
        ''')
        by_type = {row['document_type']: row['count'] for row in cursor.fetchall()}
        
        # Recent ingestions
        cursor.execute('''
            SELECT document_name, document_type, ingested_at
            FROM ingestion_log
            WHERE status = 'success'
            ORDER BY ingested_at DESC
            LIMIT 10
        ''')
        recent = [dict(row) for row in cursor.fetchall()]
        
        db.close()
        
        return {
            'total_extracts': total_extracts,
            'total_patterns': total_patterns,
            'by_document_type': by_type,
            'recent_ingestions': recent
        }


# Singleton
_ingestor = None

def get_document_ingestor() -> DocumentIngestor:
    """Get singleton document ingestor"""
    global _ingestor
    if _ingestor is None:
        _ingestor = DocumentIngestor()
    return _ingestor


# I did no harm and this file is not truncated
